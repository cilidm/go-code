# 1.集群搭建

## 预备工作

### 版本

* kubernetes v1.16
* docker 18.06.2-ce: [官方文档](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)推荐此版本
* centos 7

```
# 查看 centos 版本号
$ cat /etc/redhat-release
CentOS Linux release 7.5.1804 (Core)
```

### 节点命名

以下两个节点(云服务器)作为 k8s 的一部分

* shanyue master 172.17.68.39 (2vCPU, 4G Mem)
* shuifeng work 172.17.68.40 (2vCPU, 16G Mem)

另外有一个可以访问谷歌的节点，用以下载一些国内无法下载的镜像，把它叫做 proxy

* proxy

为了更方便且更可读画地访问服务器进行操作，在客户端机器(Mac)编辑 `~/.ssh/config` 添加以下内容

```
# ~/.ssh/config

Host shanyue
    HostName 111.111.111.111
    User root
Host shuifeng
    HostName 222.222.222.222
    User root
Host proxy
    HostName 123.123.123.123
    User root
```

此时 `ssh proxy` 可以快速登录 `root@123.123.123.123`。`rsync -avhzP k8s.tar shanyue:/root` 代表把本地的`k8s.tar`文件传输到`shanyue`目标机的`/root`目录下。

## 搭建过程中的linux命令

安装 kubernetes 是一个技术活，其中涉及到很多在 linux 上的命令行操作。在此之前，熟悉一些在 centos 上的基本命令是必不可少的。

也可以反过来说 不了解这些命令你有可能寸步难行。 以下是在安装过程中有可能会反复使用的命令

### yum

yum 是在 centos 下的包管理工具

```
# 安装 docker
$ yum install docker-ce
$ yum list | grep docker
$ yum remove docker-ce

# 删除 package
$ yum erase docker-ce

# 清除 package 的依赖
$ yum autoremove
```

### rsync

`rsync` 被用来与远程服务器间传送文件，与 `scp` 类似，但 `rsync` 可以实现增量传送

如需要传输k8s所需要的镜像压缩包时: 首次打包了总共5个镜像为 `k8s.tar`，传输到目标节点。随后发现一个镜像没有打包，于是再次打包，总共6个镜像为 `k8s.tar`，再次传输到目标节点。`rsync` 可以对比文件差异而做到仅传输最后一次丢掉的镜像。

```
# 把本机的 k8s.tar 传送到 shanyue 服务器的 /root 目录
# -a 代表归档
# -v 代表打印详细信息，这个参数很常见了 --verbose
# -h 代表打印可读性好的信息，这个参数也很常见 --human-readable
# -z 代表打包传送，减小传送体积
$ rsync -avhzP k8s.tar shanyue:/root
```

### systemctl

`systemctl` 管理 `centos` 中的服务

```
systemctl status docker
```

### journalctl

`journalctl` 用以管理 `centos` 中服务的日志。

```
$ journalctl -xeu docker
```

### route/ip

在设置网络时，可以使用该命令检查路由表。在搭建 `k8s` 集群时，可能出现 `connect: no route to host` 类似的错误，可以使用 `route -n` 或者 `ip route` 进行诊断。

```
# 打印路由表
# -n 代表使用数字代表 IP 地址
$ route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         172.17.79.253   0.0.0.0         UG    0      0        0 eth0
10.244.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0
10.244.1.0      10.244.1.0      255.255.255.0   UG    0      0        0 flannel.1
169.254.0.0     0.0.0.0         255.255.0.0     U     1002   0        0 eth0
172.17.64.0     0.0.0.0         255.255.240.0   U     0      0        0 eth0
172.18.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0

# 另外，也可以通过该命令检查其路由表
$ ip route
```

### iptables

iptables 用以控制 IP 数据报，比如转发，丢弃与过滤

```
$ iptables -L
Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain FORWARD (policy DROP)
target     prot opt source               destination
DOCKER-USER  all  --  anywhere             anywhere
DOCKER-ISOLATION-STAGE-1  all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
DOCKER     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain DOCKER (1 references)
target     prot opt source               destination

Chain DOCKER-ISOLATION-STAGE-1 (1 references)
target     prot opt source               destination
DOCKER-ISOLATION-STAGE-2  all  --  anywhere             anywhere
RETURN     all  --  anywhere             anywhere

Chain DOCKER-ISOLATION-STAGE-2 (1 references)
target     prot opt source               destination
DROP       all  --  anywhere             anywhere
RETURN     all  --  anywhere             anywhere

Chain DOCKER-USER (1 references)
target     prot opt source               destination
RETURN     all  --  anywhere             anywhere
```

### lsmod

显示已载入linux内核的模块，如以下常见的模块

* ip\_tables
* overlay
* bridge
* br\_netfilter

### modprobe

添加或删除linux内核模块

### sysctl

sysctl 用以控制内核的参数

```
# 打印所有配置
$ sysctl -a
```

## docker安装与配置

在 k8s 支持的容器方案中除了 docker，还有其它容器方案

* CRI-O
* Containerd

官方推荐的 docker 版本为: docker 18.06.2。在 k8s 的 master 与 node 节点都需要安装 docker。

### 安装 docker

> 在 centos 上安装 docker 的官方文档: [https://docs.docker.com/install/linux/docker-ce/centos/](https://docs.docker.com/engine/install/centos/)

```
$ yum install -y yum-utils device-mapper-persistent-data lvm2

# 安装 docker 官方的镜像源
$ yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

# 如果在国内，安装阿里云的镜像
$ yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

# 安装 docker
$ yum install -y docker-ce

# 安装指定版本号的 docker，以下是 k8s 官方推荐的 docker 版本号 (此时，k8s 的版本号在 v1.16)
$ yum install -y docker-ce-18.06.2.ce

$ systemctl enable docker
Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.

$ systemctl start docker
```

当 docker 安装成功后，可以使用以下命令查看版本号

```
$ docker --version
Docker version 18.06.2-ce, build 6d37f41

# 查看更详细的版本号信息
$ docker version

# 查看docker的详细配置信息
$ docker info
```

### docker daemon

dockerd 是 docker 的后台进程，而 dockerd 可以通过配置文件进行配置，在 linux 下在 /etc/docker/daemon.json，详细可以参考[官方文档](https://docs.docker.com/engine/reference/commandline/dockerd/)

```
$ mkdir /etc/docker

# 设置 docker daemon
$ cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

# 重启 docker
$ systemctl daemon-reload
$ systemctl restart docker
```

## kubeadm 简介与安装

先来介绍这三个命令行工具的作用:

* kubeadm: 用以构建一个 k8s 集群的官方工具
* kubelet: 工作在集群的每个节点，负责容器的一些行为如启动
* kubectl: 用以控制集群的客户端工具

在 k8s 的 master 与 node 节点均需要安装 kubeadm

以下使用 [阿里源](https://developer.aliyun.com/mirror/) 来安装 `kubeadm`

> 可能会有索引gpg检查失败的情况, 这时请用 `yum install -y --nogpgcheck kubelet kubeadm kubectl` 安装

```
# 安装 kubernetes.repo 的源，这里使用阿里云的源
$ cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 禁掉 SELinux
$ setenforce 0
setenforce: SELinux is disabled

$ sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# 安装关键软件
# 如果 gpg 校验失败，添加参数：yum install --nogpgcheck
$ yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

...
Complete!

# 开启服务
$ systemctl enable kubelet && systemctl start kubelet
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service

# 查看 kubelet 服务的状态
$ systemctl status kubelet
```

在 centos 中，你还需要设置 net.bridge.bridge-nf-call-iptables 为 1

```
# 更改内核配置
$ cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

# 查看更改是否生效
$ sysctl --system | grep tables
```

如果设置失败，查看你是否加载了 `br-netfiler` 的内核模块。

```
# 查看是否加载模块
$ lsmod | grep netf
br_netfilter           22256  0
bridge                151336  1 br_netfilter

# 如果没有加载，则手动加载改模块
$ modprobe br_netfilter
```

## 搭建第一个 kubernetes 集群

主节点以前叫 `master node`，现在官网称 `control plane node`。

使用 `kubeadm init` 就可以很简单地搭建一个主节点。但是在搭建主节点过程中，有可能由于国内网络的原因而不得成功。所以本篇文章分为两部分

* 如果有网络问题，如何准备离线镜像
* 搭建主节点

### 准备离线镜像

> 如果你能够访问谷歌，则直接查看下一节：搭建主节点

在 master 节点执行命令，获取需要预先下载的镜像列表。

```
# 获取预先下载好的镜像列表
$ kubeadm config images list
W1002 21:48:28.382907   14218 version.go:101] could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable-1.txt": Get https://dl.k8s.io/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
W1002 21:48:28.382998   14218 version.go:102] falling back to the local client version: v1.16.0
k8s.gcr.io/kube-apiserver:v1.16.0
k8s.gcr.io/kube-controller-manager:v1.16.0
k8s.gcr.io/kube-scheduler:v1.16.0
k8s.gcr.io/kube-proxy:v1.16.0
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.3.15-0
k8s.gcr.io/coredns:1.6.2
```

在 `proxy` 节点，把镜像列表存为 `images.txt`，通过 `docker pull` 与 `docker save` 批量下载镜像与打包。并通过 `rsync` 在代理节点与 `master/work` 节点之间传送。关于 `rsync` 的用法可以参考:

> 此时，`images.txt` 除了关于搭建集群所需要的镜像外，还有一些因网络问题而不可达的镜像。如 `dashboard/metrics-server/tiller/ingress` 等。

```
# 以下操作在 proxy 节点进行操作

# 查看所需要的镜像
$ cat images.txt
k8s.gcr.io/kube-apiserver:v1.16.0
k8s.gcr.io/kube-controller-manager:v1.16.0
k8s.gcr.io/kube-scheduler:v1.16.0
k8s.gcr.io/kube-proxy:v1.16.0
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.3.15-0
k8s.gcr.io/coredns:1.6.2

k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
k8s.gcr.io/metrics-server-amd64:v0.3.6
k8s.gcr.io/defaultbackend-amd64:1.5

# 在代理节点拉取所有镜像
# -I {}，指以 {} 代替 pipe 前每一行内容
$ cat images.txt | xargs -I {} docker pull {}

# 在代理节点打包所有镜像
$ cat images.txt | xargs docker save -o k8s.tar
```

待在 proxy 节点完成镜像打包后，使用 rsync 传输到 master 节点。以下操作在 master 节点进行

```
# 以下操作在 master 节点进行

# 复制代理节点的打包镜像到 master 节点
# proxy:/path/k8s.tar: 打包镜像在proxy节点的位置
$ rsync -avzhP proxy:/path/k8s.tar .

# 加载 k8s.tar 中所有镜像
# load，从压缩文件中加载镜像
# -i，指定压缩文件
$ docker load -i k8s.tar
```

### 搭建主节点

当搭建主节点时，你需要在 gcr.io 上拉取所需镜像，但 gcr.io 有可能网络不通，你可以通过以下命令测试下连接性

```
# 如果有以下提示，代表连接不通
$ kubeadm config images pull
W0905 19:04:37.519303   11952 version.go:98] could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable-1.txt": Get https://dl.k8s.io/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
W0905 19:04:37.519384   11952 version.go:99] falling back to the local client version: v1.15.3
```

> 如果你不能获取镜像的话，可以通过准备离线镜像来获取。具体参考上一小节

测试成功后，使用 `kubeadm init` 命令添加一个主节点 (`control-plane node`)。

* 172.17.68.39 指 master 节点的IP地址，可以通过 ifconfig eth0 获得
* 59.110.216.155 指 master 节点的公网IP

```
# init: 初始化一个 master 节点，现在也叫 control plane node (控制面板节点)
# --apiserver-advertise-address: 可以视作主节点的 IP 地址，这里是 172.17.68.39
# --pod-network-cidr: 当使用 pod network 时需要指定，用以 pod 间互相通信
# --apiserver-cert-extra-sans: 证书白名单，如果你使用 VPN 的话，可以不指定该参数 TODO
$ kubeadm init --apiserver-advertise-address=172.17.68.39 --pod-network-cidr=10.244.0.0/16 --apiserver-cert-extra-sans=59.110.216.155
...
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.17.68.39:6443 --token qq8hbl.4utma949mu0p47v4 \
    --discovery-token-ca-cert-hash sha256:cce6cd7ec86cf4cd65215bea554f98c786783720b19262533cd98656ac6eb15e
```

到这里为止，k8s 集群已经初步搭建完成。不过你会有疑问，在 kubeadm init 的过程中做了什么，这都被它作为日志打印了出来

接下来你可以按照以上输出的指示做完以下命令，这将生成一个 kubectl 的配置文件，以及检查集群状态

```
$ mkdir -p $HOME/.kube
$ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ chown $(id -u):$(id -g) $HOME/.kube/config

$ kubectl cluster-info
Kubernetes master is running at https://172.17.68.39:6443
KubeDNS is running at https://172.17.68.39:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

$ kubectl get pods --all-namespaces
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE
kube-system   coredns-5644d7b6d9-8l2gv          0/1     Pending   0          56m
kube-system   coredns-5644d7b6d9-l8zv5          0/1     Pending   0          56m
kube-system   etcd-shanyue                      1/1     Running   0          55m
kube-system   kube-apiserver-shanyue            1/1     Running   0          55m
kube-system   kube-controller-manager-shanyue   1/1     Running   0          55m
kube-system   kube-proxy-5drlg                  1/1     Running   0          56m
kube-system   kube-scheduler-shanyue            1/1     Running   0          55m
```

再往后，你也可以通过以上的输出指示添加 worker node

```
$ kubeadm join 172.17.68.39:6443 --token qq8hbl.4utma949mu0p47v4 \
    --discovery-token-ca-cert-hash sha256:cce6cd7ec86cf4cd65215bea554f98c786783720b19262533cd98656ac6eb15e
```

### kubectl 命令自动补全

没有自动补全的 kubectl 就如同没带眼镜的近视者，可以使用，但很难受。

```
# 安装自动补全插件
$ yum install bash-completion

# 添加 kubectl 自动补全的脚本至 /etc/bash_completion.d 目录下
# kubectl completion bash: 生成自动补全的脚本
$ kubectl completion bash >/etc/bash_completion.d/kubectl

# 自动补全生效
$ kubectl get n
namespaces                         networkpolicies.extensions         networkpolicies.networking.k8s.io  nodes
```

## 为集群添加一个工作节点

### 安装 network pod: flannel

网络组件用以在 pod 间进行通信，再此之前，我们会发现 coredns 组件处于 Pending 状态。我们使用网络组件 flannel 来确保 coredns 的正常运行。

```
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE
kube-system   coredns-5644d7b6d9-8l2gv          0/1     Pending   0          56m
kube-system   coredns-5644d7b6d9-l8zv5          0/1     Pending   0          56m
kube-system   etcd-shanyue                      1/1     Running   0          55m
kube-system   kube-apiserver-shanyue            1/1     Running   0          55m
kube-system   kube-controller-manager-shanyue   1/1     Running   0          55m
kube-system   kube-proxy-5drlg                  1/1     Running   0          56m
kube-system   kube-scheduler-shanyue            1/1     Running   0          55m
```

在安装网络组件要确保路是通的，使用 `sysctl` 设置内核变量 `net.bridge.bridge-nf-call-iptables` 为1

```
$ sysctl net.bridge.bridge-nf-call-iptables=1

$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created
```

此时，再次查看集群中所有的 pod 状态，此时 coredn 变为正常状态，且多了 kube-flannel-ds-amd64 这个 pod。

```
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE
kube-system   coredns-5644d7b6d9-8l2gv          1/1     Running   0          136m
kube-system   coredns-5644d7b6d9-l8zv5          1/1     Running   0          136m
kube-system   etcd-shanyue                      1/1     Running   0          136m
kube-system   kube-apiserver-shanyue            1/1     Running   0          135m
kube-system   kube-controller-manager-shanyue   1/1     Running   0          136m
kube-system   kube-flannel-ds-amd64-pmlnw       1/1     Running   0          9m23s
kube-system   kube-proxy-5drlg                  1/1     Running   0          136m
kube-system   kube-scheduler-shanyue            1/1     Running   0          136m
```

### 添加 worker node

在添加 worker node 时，需要在子节点也进行 docker 以及 kubeadm 的安装，按照以上章节步骤进行安装。

安装之后根据以上关于搭建主节点章节的输出指示，使用 `kubeadm join` 加入集群之中:

```
$ kubeadm join 172.17.68.39:6443 --token qq8hbl.4utma949mu0p47v4 \
    --discovery-token-ca-cert-hash sha256:cce6cd7ec86cf4cd65215bea554f98c786783720b19262533cd98656ac6eb15e
[preflight] Running pre-flight checks
        [WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.16" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Activating the kubelet service
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

`kubeadm join` 有两个关键的参数: `token` 与 `hash`。如果你已经丢失了新建 `master node` 时的输出 `kubeadm join` 信息怎么办？此时可以通过以下命令来获取

```
# 以下操作在 master node
# kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>

# 获取 token
$ kubectl token list

# 获取 hash
$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
```

安装完之后，再次打印节点信息

```
$ kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
shanyue    Ready    master   17m   v1.16.0
shuifeng   Ready    <none>   15m   v1.16.0

$ kubectl get pods --all-namespaces
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE
kube-system   coredns-5644d7b6d9-845lr          1/1     Running   0          24m
kube-system   coredns-5644d7b6d9-k6dqm          1/1     Running   0          24m
kube-system   etcd-shanyue                      1/1     Running   0          23m
kube-system   kube-apiserver-shanyue            1/1     Running   0          23m
kube-system   kube-controller-manager-shanyue   1/1     Running   0          23m
kube-system   kube-flannel-ds-amd64-tdvbs       1/1     Running   0          21m
kube-system   kube-flannel-ds-amd64-vtrnh       1/1     Running   0          21m
kube-system   kube-proxy-k46l2                  1/1     Running   0          24m
kube-system   kube-proxy-rhrrg                  1/1     Running   0          22m
kube-system   kube-scheduler-shanyue            1/1     Running   0          23m
```

至此，一个拥有 `master node` 与 `worker node` 的 `kubernetes` 集群就搭建完成了。
